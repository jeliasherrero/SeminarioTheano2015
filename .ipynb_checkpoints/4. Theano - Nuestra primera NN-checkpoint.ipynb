{
 "metadata": {
  "name": "",
  "signature": "sha256:aa7d76b6b9bf34f1c5137bdcf75a01cc239b4884699b2ce202d2303559d4a58b"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Theano - Nuestra primera red neuronal"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "En este caso, vamos a trabajar con los mismos datos a los utilizados en el ejemplo de Logistic Regression pero usando un perceptr\u00f3n multi-capa.\n",
      "\n",
      "Cargamos los datos de igual forma que antes:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy\n",
      "import theano\n",
      "import theano.tensor as T\n",
      "import scipy.io as io\n",
      "        \n",
      "print '... cargando datos'\n",
      "data=io.loadmat('dataLR.mat',squeeze_me=True)\n",
      "dataIn=data['data'][:,0:2].astype(theano.config.floatX)\n",
      "dataOut = data['data'][:,2].astype(int)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Clase CapaOculta"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class CapaOculta(object):\n",
      "    def __init__(self, rng, input, n_in, n_out, W=None, b=None,\n",
      "                 activation=T.tanh):\n",
      "        \"\"\"\n",
      "        Capa oculta t\u00edpica de un MLP: las neuronas est\u00e1n todas conectadas y tienen una funci\u00f3n de activaci\u00f3n simoidea.\n",
      "        La matriz de pesos \"W\" es de la forma (n_in,n_out)\n",
      "        y el vector bias \"b\" (n_out,).\n",
      "\n",
      "        Nota : Usamos TANH\n",
      "\n",
      "        La funci\u00f3n de activaci\u00f3n viene dada por: tanh(dot(input,W) + b)\n",
      "\n",
      "        :type rng: numpy.random.RandomState\n",
      "        :param rng: Generador de n\u00famero aleatorios para inicializar los pesos\n",
      "\n",
      "        :type input: theano.tensor.dmatrix\n",
      "        :param input: Un tensor simb\u00f3lico para definir los datos de entrada (n_examples, n_in)\n",
      "\n",
      "        :type n_in: int\n",
      "        :param n_in: dimensionalidad de la entrada\n",
      "\n",
      "        :type n_out: int\n",
      "        :param n_out: n\u00famero de neuronas ocultas\n",
      "\n",
      "        :type activation: theano.Op or function\n",
      "        :param activation: Funci\u00f3n usada en la capa oculta\n",
      "        \"\"\"\n",
      "        self.input = input\n",
      "       \n",
      "        if W is None:\n",
      "            W_values = numpy.asarray(\n",
      "                rng.uniform(\n",
      "                    low=-numpy.sqrt(6. / (n_in + n_out)),\n",
      "                    high=numpy.sqrt(6. / (n_in + n_out)),\n",
      "                    size=(n_in, n_out)\n",
      "                ),\n",
      "                dtype=theano.config.floatX  # @UndefinedVariable\n",
      "            )\n",
      "            if activation == T.nnet.sigmoid:\n",
      "                W_values *= 4\n",
      "\n",
      "            W = theano.shared(value=W_values, name='W', borrow=True)\n",
      "\n",
      "        if b is None:\n",
      "            b_values = numpy.zeros((n_out,), dtype=theano.config.floatX)\n",
      "            b = theano.shared(value=b_values, name='b', borrow=True)\n",
      "\n",
      "        self.W = W\n",
      "        self.b = b\n",
      "\n",
      "        lin_output = T.dot(input, self.W) + self.b\n",
      "        self.output = (\n",
      "            lin_output if activation is None\n",
      "            else activation(lin_output)\n",
      "        )\n",
      "        #Par\u00e1metros del modelo\n",
      "        self.params = [self.W, self.b]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Clase LogisticRegression"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class LogisticRegression(object):\n",
      "    \"\"\"Multi-class Logistic Regression Class\n",
      "\n",
      "    The logistic regression is fully described by a weight matrix :math:`W`\n",
      "    and bias vector :math:`b`. Classification is done by projecting data\n",
      "    points onto a set of hyperplanes, the distance to which is used to\n",
      "    determine a class membership probability.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, input, n_in, n_out):\n",
      "        \"\"\" Initialize the parameters of the logistic regression\n",
      "\n",
      "        :type input: theano.tensor.TensorType\n",
      "        :param input: symbolic variable that describes the input of the\n",
      "                      architecture (one minibatch)\n",
      "\n",
      "        :type n_in: int\n",
      "        :param n_in: number of input units, the dimension of the space in\n",
      "                     which the datapoints lie\n",
      "\n",
      "        :type n_out: int\n",
      "        :param n_out: number of output units, the dimension of the space in\n",
      "                      which the labels lie\n",
      "\n",
      "        \"\"\"\n",
      "        # start-snippet-1\n",
      "        # initialize with 0 the weights W as a matrix of shape (n_in, n_out)\n",
      "        self.W = theano.shared(\n",
      "            value=numpy.zeros(\n",
      "                (n_in, n_out),\n",
      "                dtype=theano.config.floatX\n",
      "            ),\n",
      "            name='W',\n",
      "            borrow=True\n",
      "        )\n",
      "        # initialize the baises b as a vector of n_out 0s\n",
      "        self.b = theano.shared(\n",
      "            value=numpy.zeros(\n",
      "                (n_out,),\n",
      "                dtype=theano.config.floatX\n",
      "            ),\n",
      "            name='b',\n",
      "            borrow=True\n",
      "        )\n",
      "\n",
      "        # symbolic expression for computing the matrix of class-membership\n",
      "        # probabilities\n",
      "        # Where:\n",
      "        # W is a matrix where column-k represent the separation hyper plain for\n",
      "        # class-k\n",
      "        # x is a matrix where row-j  represents input training sample-j\n",
      "        # b is a vector where element-k represent the free parameter of hyper\n",
      "        # plain-k\n",
      "        self.p_y_given_x = T.nnet.softmax(T.dot(input, self.W) + self.b)\n",
      "\n",
      "        # symbolic description of how to compute prediction as class whose\n",
      "        # probability is maximal\n",
      "        self.y_pred = T.argmax(self.p_y_given_x, axis=1)\n",
      "        # end-snippet-1\n",
      "\n",
      "        # parameters of the model\n",
      "        self.params = [self.W, self.b]\n",
      "\n",
      "    def negative_log_likelihood(self, y):\n",
      "        \"\"\"Return the mean of the negative log-likelihood of the prediction\n",
      "        of this model under a given target distribution.\n",
      "\n",
      "        .. math::\n",
      "\n",
      "            \\frac{1}{|\\mathcal{D}|} \\mathcal{L} (\\theta=\\{W,b\\}, \\mathcal{D}) =\n",
      "            \\frac{1}{|\\mathcal{D}|} \\sum_{i=0}^{|\\mathcal{D}|}\n",
      "                \\log(P(Y=y^{(i)}|x^{(i)}, W,b)) \\\\\n",
      "            \\ell (\\theta=\\{W,b\\}, \\mathcal{D})\n",
      "\n",
      "        :type y: theano.tensor.TensorType\n",
      "        :param y: corresponds to a vector that gives for each example the\n",
      "                  correct label\n",
      "\n",
      "        Note: we use the mean instead of the sum so that\n",
      "              the learning rate is less dependent on the batch size\n",
      "        \"\"\"\n",
      "        # start-snippet-2\n",
      "        # y.shape[0] is (symbolically) the number of rows in y, i.e.,\n",
      "        # number of examples (call it n) in the minibatch\n",
      "        # T.arange(y.shape[0]) is a symbolic vector which will contain\n",
      "        # [0,1,2,... n-1] T.log(self.p_y_given_x) is a matrix of\n",
      "        # Log-Probabilities (call it LP) with one row per example and\n",
      "        # one column per class LP[T.arange(y.shape[0]),y] is a vector\n",
      "        # v containing [LP[0,y[0]], LP[1,y[1]], LP[2,y[2]], ...,\n",
      "        # LP[n-1,y[n-1]]] and T.mean(LP[T.arange(y.shape[0]),y]) is\n",
      "        # the mean (across minibatch examples) of the elements in v,\n",
      "        # i.e., the mean log-likelihood across the minibatch.\n",
      "        return -T.mean(T.log(self.p_y_given_x)[T.arange(y.shape[0]), y])\n",
      "        # end-snippet-2\n",
      "\n",
      "    def errors(self, y):\n",
      "        \"\"\"Return a float representing the number of errors in the minibatch\n",
      "        over the total number of examples of the minibatch ; zero one\n",
      "        loss over the size of the minibatch\n",
      "\n",
      "        :type y: theano.tensor.TensorType\n",
      "        :param y: corresponds to a vector that gives for each example the\n",
      "                  correct label\n",
      "        \"\"\"\n",
      "\n",
      "        # check if y has same dimension of y_pred\n",
      "        if y.ndim != self.y_pred.ndim:\n",
      "            raise TypeError(\n",
      "                'y should have the same shape as self.y_pred',\n",
      "                ('y', y.type, 'y_pred', self.y_pred.type)\n",
      "            )\n",
      "        # check if y is of the correct datatype\n",
      "        if y.dtype.startswith('int'):\n",
      "            # the T.neq operator returns a vector of 0s and 1s, where 1\n",
      "            # represents a mistake in prediction\n",
      "            return T.mean(T.neq(self.y_pred, y))\n",
      "        else:\n",
      "            raise NotImplementedError()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Clase MLP"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class MLP(object):\n",
      "    \"\"\"Clase Perceptr\u00f3n multicapa\n",
      "\n",
      "    Vamos a definir una sola capa oculta usando la clase CapaOculta que hemos creado anteriormente, y usaremos una capa de\n",
      "    salida tipo softmax para la que usaremos la clase LogisticRegression.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, rng, input, n_in, n_hidden, n_out):\n",
      "        \"\"\"Initialize the parameters for the multilayer perceptron\n",
      "\n",
      "        :type rng: numpy.random.RandomState\n",
      "        :param rng: Generador de n\u00famero aleatorios para la inicializaci\u00f3n de los pesos\n",
      "\n",
      "        :type input: theano.tensor.TensorType\n",
      "        :param input: Variable simb\u00f3lica para la entrada al MLP\n",
      "\n",
      "        :type n_in: int\n",
      "        :param n_in: N\u00famero de neuronas de entrada\n",
      "\n",
      "        :type n_hidden: int\n",
      "        :param n_hidden: N\u00famero de neuronas en la capa oculta\n",
      "\n",
      "        :type n_out: int\n",
      "        :param n_out: N\u00famero de neuronas en la capa de salida\n",
      "\n",
      "        \"\"\"\n",
      "\n",
      "        # Creamos la capa oculta\n",
      "        self.hiddenLayer = CapaOculta(\n",
      "            rng=rng,\n",
      "            input=input,\n",
      "            n_in=n_in,\n",
      "            n_out=n_hidden,\n",
      "            activation=T.tanh\n",
      "        )\n",
      "\n",
      "        # La capa LR tendr\u00e1 como entrada las neuronas de la capa oculta\n",
      "        self.logRegressionLayer = LogisticRegression(\n",
      "            input=self.hiddenLayer.output,\n",
      "            n_in=n_hidden,\n",
      "            n_out=n_out\n",
      "        )\n",
      "        # end-snippet-2 start-snippet-3\n",
      "        # L1 norm: Nos sirve para regularizar.\n",
      "        self.L1 = (\n",
      "            abs(self.hiddenLayer.W).sum()\n",
      "            + abs(self.logRegressionLayer.W).sum()\n",
      "        )\n",
      "\n",
      "        # square of L2 norm: otra forma de regularizar.\n",
      "        self.L2_sqr = (\n",
      "            (self.hiddenLayer.W ** 2).sum()\n",
      "            + (self.logRegressionLayer.W ** 2).sum()\n",
      "        )\n",
      "\n",
      "        # Return the mean of the negative log-likelihood of the prediction\n",
      "        # of this model under a given target distribution.\n",
      "        self.negative_log_likelihood = (\n",
      "            self.logRegressionLayer.negative_log_likelihood\n",
      "        )\n",
      "        # Almacenamos los errores\n",
      "        self.errors = self.logRegressionLayer.errors\n",
      "\n",
      "        # Guardamos como par\u00e1metros los par\u00e1metros de las dos capas\n",
      "        self.params = self.hiddenLayer.params + self.logRegressionLayer.params"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Algoritmo de testeo"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "learning_rate=0.1\n",
      "L1_reg=0.00\n",
      "L2_reg=0.0001\n",
      "n_epochs=10000\n",
      "batch_size=20\n",
      "n_hidden=10\n",
      "\n",
      "train_set_x = theano.shared(dataIn)\n",
      "train_set_y = theano.shared(dataOut)\n",
      "n_train_batches = train_set_x.get_value(borrow=True).shape[0] / batch_size\n",
      "    \n",
      "print '... building the model'\n",
      "    \n",
      "index = T.iscalar()  # \u00cdndice del lote\n",
      "x = T.matrix('x')  # Datos de entrada\n",
      "y = T.lvector('y')  # Datos de salida esperados\n",
      "\n",
      "rng = numpy.random.RandomState(1234)\n",
      "\n",
      "# Construimos el objeto MLP\n",
      "classifier = MLP(\n",
      "    rng=rng,\n",
      "    input=x,\n",
      "    n_in=2,\n",
      "    n_hidden=n_hidden,\n",
      "    n_out=1\n",
      ")\n",
      "    \n",
      "# Funci\u00f3n de coste a minimizar\n",
      "cost = (\n",
      "        classifier.negative_log_likelihood(y)\n",
      "        + L1_reg * classifier.L1\n",
      "        + L2_reg * classifier.L2_sqr\n",
      ")\n",
      "    \n",
      "# Calculamos el gradiente de la funci\u00f3n de coste con respecto a los par\u00e1metros de las dos capas\n",
      "gparams = [T.grad(cost, param) for param in classifier.params]\n",
      "\n",
      "# Y definimos las actualizaciones de los par\u00e1metros\n",
      "updates = [\n",
      "        (param, param - learning_rate * gparam)\n",
      "        for param, gparam in zip(classifier.params, gparams)\n",
      "]\n",
      "\n",
      "print train_set_x.dtype\n",
      "print train_set_y.dtype\n",
      "print index.dtype\n",
      "# Compilamos la funci\u00f3n de aprendizaje\n",
      "train_model = theano.function(\n",
      "    inputs=[index],\n",
      "    outputs=cost,\n",
      "    updates=updates,\n",
      "    givens={\n",
      "        x: train_set_x[index * batch_size: (index + 1) * batch_size],\n",
      "        y: train_set_y[index * batch_size: (index + 1) * batch_size]\n",
      "    }\n",
      ")\n",
      "    \n",
      "print '... entrenando'\n",
      "epoch = 0\n",
      "while (epoch < n_epochs):\n",
      "        epoch = epoch + 1\n",
      "        minibatch_avg_cost = 0\n",
      "        for minibatch_index in xrange(n_train_batches):\n",
      "            minibatch_avg_cost = minibatch_avg_cost + train_model(minibatch_index)\n",
      "        print \"\u00c9poca: \" + repr(epoch) + \" - Error medio: \" + repr(minibatch_avg_cost/n_train_batches)\n",
      "\n",
      "predict = theano.function(\n",
      "    inputs=[index],\n",
      "    outputs=classifier.logRegressionLayer.y_pred,\n",
      "    givens={\n",
      "        x: train_set_x[index * batch_size: (index + 1) * batch_size]\n",
      "    }\n",
      ")\n",
      "\n",
      "test = [predict(i) for i\n",
      "        in xrange(n_train_batches)]\n",
      "print test"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}