{
 "metadata": {
  "name": "",
  "signature": "sha256:f8e1b775e90641cce89957a8b3dd8dd334e1cf3e4a221e82492152c038531812"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Stacked Auto-Encoders"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Este Notebook introduce el concepto de los stacked autoencoders. Para ello necesitamos crear una clase que codifique una capa del tipo Denoising Auto-Encoders."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Denoising Auto-Encoders"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Una capa Auto-Encoder dispone de un encoder y un decoder que realizan el aprendizaje de los patrones de entrada. B\u00e1sicamente se puede expresar esto matem\u00e1ticamente como:"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Encoder:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\\begin{align}\n",
      "y &= s(W*x +b)\n",
      "\\end{align}\n",
      "\n",
      "Siendo s la funci\u00f3n sigmoidea, por ejemplo."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Decoder:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\\begin{align}\n",
      "z &= s(W'*x+b')\n",
      "\\end{align}\n",
      "\n",
      "W' ser\u00eda el mapeo inverso, y z ser\u00eda la reconstrucci\u00f3n de x (datos de entrada). Una forma de calcular W' ser\u00eda:\n",
      "\n",
      "\\begin{align}\n",
      "W' &= W^T\n",
      "\\end{align}\n",
      "\n",
      "Con lo cual tenemos tres par\u00e1metros en esta capa:\n",
      "\n",
      "\\begin{align}\n",
      "{W, b, b'}\n",
      "\\end{align}"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Error de reconstrucci\u00f3n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Se puede calcular de muchas formas, pero de forma sencilla se puede definir como la <strong>entrop\u00eda cruzada</strong>:\n",
      "\n",
      "\\begin{align}\n",
      "L_H(x,z) &= -\\sum_{k=1}^d[X_k*log(Z_k) + (1-X_k)*log(1-Z_k)]\n",
      "\\end{align}"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "A\u00f1adiendo la capacidad \"denoising\""
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Si utilizamos el esquema anterior, el Auto-Encoder representar\u00e1 la identificaci\u00f3n \u00fanicamente de los datos de entrada. Pero es mucho m\u00e1s ventajoso que el Auto-Encoder extraiga las representaciones o caracter\u00edsticas m\u00e1s representativas de los datos.\n",
      "\n",
      "Una sencilla t\u00e9cnica es introducir una serie de distorsiones, o m\u00e1s bien dicho, corrompemos la entrada de una determinada forma. Por ejemplo, eliminando alguno de los datos de entrada (puestos a cero). En theano esto se puede realizar con la funci\u00f3n theano_rng.binomial, que devuelve un vector con 1s y 0s, usados para filtrar/corromper los datos de entrada.\n",
      "\n",
      "<img src=\"./images/dA.png\">"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Codificaci\u00f3n en Theano"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class dA(object):\n",
      "    \"\"\"Denoising Auto-Encoder class (dA)\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        numpy_rng,\n",
      "        theano_rng=None,\n",
      "        input=None,\n",
      "        n_visible=784,\n",
      "        n_hidden=500,\n",
      "        W=None,\n",
      "        bhid=None,\n",
      "        bvis=None\n",
      "    ):\n",
      "        self.n_visible = n_visible\n",
      "        self.n_hidden = n_hidden\n",
      "\n",
      "        # create a Theano random generator that gives symbolic random values\n",
      "        if not theano_rng:\n",
      "            theano_rng = RandomStreams(numpy_rng.randint(2 ** 30))\n",
      "\n",
      "        # note : W' was written as `W_prime` and b' as `b_prime`\n",
      "        if not W:\n",
      "            initial_W = numpy.asarray(\n",
      "                numpy_rng.uniform(\n",
      "                    low=-4 * numpy.sqrt(6. / (n_hidden + n_visible)),\n",
      "                    high=4 * numpy.sqrt(6. / (n_hidden + n_visible)),\n",
      "                    size=(n_visible, n_hidden)\n",
      "                ),\n",
      "                dtype=theano.config.floatX\n",
      "            )\n",
      "            W = theano.shared(value=initial_W, name='W', borrow=True)\n",
      "\n",
      "        if not bvis:\n",
      "            bvis = theano.shared(\n",
      "                value=numpy.zeros(\n",
      "                    n_visible,\n",
      "                    dtype=theano.config.floatX\n",
      "                ),\n",
      "                borrow=True\n",
      "            )\n",
      "\n",
      "        if not bhid:\n",
      "            bhid = theano.shared(\n",
      "                value=numpy.zeros(\n",
      "                    n_hidden,\n",
      "                    dtype=theano.config.floatX\n",
      "                ),\n",
      "                name='b',\n",
      "                borrow=True\n",
      "            )\n",
      "\n",
      "        self.W = W\n",
      "        # b corresponds to the bias of the hidden\n",
      "        self.b = bhid\n",
      "        # b_prime corresponds to the bias of the visible\n",
      "        self.b_prime = bvis\n",
      "        # tied weights, therefore W_prime is W transpose\n",
      "        self.W_prime = self.W.T\n",
      "        self.theano_rng = theano_rng\n",
      "        # if no input is given, generate a variable representing the input\n",
      "        if input is None:\n",
      "            # we use a matrix because we expect a minibatch of several\n",
      "            # examples, each example being a row\n",
      "            self.x = T.dmatrix(name='input')\n",
      "        else:\n",
      "            self.x = input\n",
      "\n",
      "        self.params = [self.W, self.b, self.b_prime]\n",
      "        \n",
      "    def get_corrupted_input(self, input, corruption_level):\n",
      "        return self.theano_rng.binomial(size=input.shape, n=1,\n",
      "                                        p=1 - corruption_level,\n",
      "                                        dtype=theano.config.floatX) * input\n",
      "\n",
      "    def get_hidden_values(self, input):\n",
      "        \"\"\" Computes the values of the hidden layer \"\"\"\n",
      "        return T.nnet.sigmoid(T.dot(input, self.W) + self.b)\n",
      "\n",
      "    def get_reconstructed_input(self, hidden):\n",
      "        \"\"\"Computes the reconstructed input given the values of the\n",
      "        hidden layer\n",
      "\n",
      "        \"\"\"\n",
      "        return T.nnet.sigmoid(T.dot(hidden, self.W_prime) + self.b_prime)\n",
      "\n",
      "    def get_cost_updates(self, corruption_level, learning_rate):\n",
      "        \"\"\" This function computes the cost and the updates for one trainng\n",
      "        step of the dA \"\"\"\n",
      "\n",
      "        tilde_x = self.get_corrupted_input(self.x, corruption_level)\n",
      "        y = self.get_hidden_values(tilde_x)\n",
      "        z = self.get_reconstructed_input(y)\n",
      "        L = - T.sum(self.x * T.log(z) + (1 - self.x) * T.log(1 - z), axis=1)\n",
      "        cost = T.mean(L)\n",
      "\n",
      "        # compute the gradients of the cost of the `dA` with respect\n",
      "        # to its parameters\n",
      "        gparams = T.grad(cost, self.params)\n",
      "        # generate the list of updates\n",
      "        updates = [\n",
      "            (param, param - learning_rate * gparam)\n",
      "            for param, gparam in zip(self.params, gparams)\n",
      "        ]\n",
      "\n",
      "        return (cost, updates)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Stacked Auto-Encoders"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "En la secci\u00f3n anterior hemos preparado la clase necesaria para incluir un Denoising Auto-Encoder (DaE) en nuestras redes.\n",
      "\n",
      "El ejemplo que proponemos en este Notebook es un MLP de dos capas ocultas y una capa de salida (Logistic Regression).\n",
      "\n",
      "Esta ser\u00e1 la red sobre la que vamos a incluir las capas DaE, es decir, un simple MLP:\n",
      "\n",
      "<img src=\"./images/MLP1.png\">\n",
      "\n",
      "Y la red resultante que obtendremos con la inclusi\u00f3n de las correspondientes capas DaE ser\u00e1 la siguiente:\n",
      "\n",
      "<img src=\"./images/MLP2.png\">\n",
      "\n",
      "Esta nueva red se convertir\u00eda en una red del tipo <strong>Stacked Auto-Encoders</strong> (SaE).\n",
      "\n",
      "En este ejemplo en concreto vamos a clasificar im\u00e1genes de 20x20 pixels de n\u00fameros escritos a mano desde el 0 al 9. Los datos de entrenamiento est\u00e1n guardados en el fichero digits.mat de matlab."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Propiedades"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<ul>\n",
      "<li>Los par\u00e1metros W y b de las capas Sigmoidea y DaE ser\u00e1n los mismos. Es decir, si modificamos los par\u00e1metros W y b de la capa DaE se modificar\u00e1n los correspondientes a la capa Sigmoidea.</li>\n",
      "<li>La base de las SaE es la realizar un pre-entrenamiento \u00fanicamente de las capas DaE, seguido de un proceso de ajuste fino en la que solo se entrenan las capas sigmoideas y la capa de salida.</li>\n",
      "</ul>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Pre-entrenamiento"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<ul>\n",
      "<li>Paso 1: Se entrena la capa {DaE_0} con los datos de entrada</li>\n",
      "<li>Paso 2: Obtener la salida de la capa {Sigmoid_0}. (Nota: Mismos W y b que {DaE_0})</li>\n",
      "<li>Paso 3: Para el resto de las capas (i = 1 to n, siendo n el n\u00famero de capas restantes):</li>\n",
      "<li>Paso 3.1: Entrenar la capa {DaE_i} con los datos de salida de la capa {Sigmoid_(i-1)}.</li>\n",
      "<li>Paso 3.2: Obtener la salida de la capa {Sigmoid_i} con los datos de salida de la capa {Sigmoid(i-1)}.</li>\n",
      "</ul>\n",
      "</ul>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "En theano:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import scipy.io as io\n",
      "\n",
      "import numpy\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "import theano\n",
      "import theano.tensor as T\n",
      "from theano.tensor.shared_randomstreams import RandomStreams\n",
      "\n",
      "from mlp import CapaOculta, LogisticRegression\n",
      "from dA import dA"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class SdA_MLP_Layer(object):\n",
      "    def __init__(self, input, numpy_rng, theano_rng=None, n_ins=784, hidden_layers_size=500, corruption_level=.1):\n",
      "\n",
      "\t\t# Tensor de entrada\n",
      "\t\tself.input = input\n",
      "\n",
      "\t\tif not theano_rng:\n",
      "\t\t\ttheano_rng = RandomStreams(numpy_rng.randint(2 ** 30))\n",
      "\n",
      "\t\t# Capa sigmoidea\n",
      "\t\tself.sigmoid_layer = CapaOculta(rng=numpy_rng, input=input, n_in=n_ins, n_out=hidden_layers_size, activation=T.nnet.sigmoid)\n",
      "\n",
      "\t\t# Capa Denoising Autoencoder\n",
      "\t\tself.dA_layer = dA(numpy_rng=numpy_rng, theano_rng=theano_rng, input=input, n_visible=n_ins, n_hidden=hidden_layers_size, W=self.sigmoid_layer.W, bhid=self.sigmoid_layer.b)\n",
      "\n",
      "\t\tself.params = self.sigmoid_layer.params\n",
      "\n",
      "\t\tself.output = self.sigmoid_layer.output\n",
      "\n",
      "\tdef pretraining(self, train_input, batch_size):\n",
      "\t\t# Creamos los tensores\n",
      "\t\tcorruption_level = T.scalar('corruption')\n",
      "\t\tlearning_rate = T.scalar('lr')\n",
      "\t\tindex = T.iscalar('index')\n",
      "\n",
      "\t\t# Calculamos los indices del lote\n",
      "\t\tbatch_begin = index * batch_size\n",
      "\t\tbatch_end = batch_begin + batch_size\n",
      "\n",
      "\t\t# Calculamos los costes y las actualizaciones\n",
      "\t\tcost, updates = self.dA_layer.get_cost_updates(corruption_level, learning_rate)\n",
      "\n",
      "\t\tfn = theano.function(\n",
      "\t\t\tinputs=[\n",
      "\t\t\t\tindex,\n",
      "\t\t\t\ttheano.Param(corruption_level, default=0.2),\n",
      "\t\t\t\ttheano.Param(learning_rate, default=0.1)],\n",
      "\t\t\toutputs=cost,\n",
      "\t\t\tupdates=updates,\n",
      "\t\t\tgivens={self.input: train_input[batch_begin:batch_end]})\n",
      "\t\treturn fn\n",
      "\n",
      "\tdef getOut(self, inputData, num_inputs):\n",
      "\t\tindex = T.iscalar('index')\n",
      "\t\tbatch_begin = index * num_inputs\n",
      "\t\tbatch_end = (index+1) * num_inputs\n",
      "\n",
      "\t\tn_train_batches = inputData.get_value(borrow=True).shape[0] / batch_size\n",
      "\n",
      "\t\tsalida = theano.function(\n",
      "\t\t\tinputs=[index],\n",
      "\t\t\toutputs=self.output,\n",
      "\t\t\tgivens={self.input: inputData[batch_begin:batch_end]})\n",
      "\t\treturn salida(0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class SdA_MLP_network(object):\n",
      "\tdef __init__(self, input, output, hidden_layers_sizes=[500, 500], corruption_levels=[0.1, 0.1], n_outs=10, n_ins=400):\n",
      "\t\t\n",
      "\t\tnumpy_rng = numpy.random.RandomState(89677)\n",
      "\n",
      "\t\tself.input = input\n",
      "\t\tself.output = output\n",
      "\n",
      "\t\tself.corruption_levels = corruption_levels\n",
      "\n",
      "\t\tself.layer0 = SdA_MLP_Layer(\n",
      "\t\t\tinput=input,\n",
      "\t\t\tnumpy_rng=numpy_rng,\n",
      "\t\t\tn_ins=n_ins,\n",
      "\t\t\thidden_layers_size=hidden_layers_sizes[0],\n",
      "\t\t\tcorruption_level=corruption_levels[0])\n",
      "\n",
      "\t\tself.layer1 = SdA_MLP_Layer(\n",
      "\t\t\tinput=self.layer0.output,\n",
      "\t\t\tnumpy_rng=numpy_rng,\n",
      "\t\t\tn_ins=hidden_layers_sizes[0],\n",
      "\t\t\thidden_layers_size=hidden_layers_sizes[1],\n",
      "\t\t\tcorruption_level=corruption_levels[1])\n",
      "\n",
      "\t\tself.log_layer = LogisticRegression(\n",
      "\t\t\tinput=self.layer1.output,\n",
      "\t\t\tn_in=hidden_layers_sizes[-1],\n",
      "\t\t\tn_out=n_outs)\n",
      "\n",
      "\t\tself.params = self.log_layer.params + self.layer0.params + self.layer1.params\n",
      "\n",
      "\t\tself.finetune_cost = self.log_layer.negative_log_likelihood(output)\n",
      "\t\tself.errors = self.log_layer.errors(output)\n",
      "\n",
      "\tdef pretraining(self, inputData, batch_size, pretraining_epochs, pretrain_lr):\n",
      "\n",
      "\t\tn_train_batches = inputData.get_value(borrow=True).shape[0] / batch_size\n",
      "\t\t\n",
      "\t\tfn0 = self.layer0.pretraining(inputData, batch_size)\n",
      "\n",
      "\t\tfor epoch in xrange(pretraining_epochs):\n",
      "\t\t\tc = []\n",
      "\t\t\tfor batch_index in xrange(n_train_batches):\n",
      "\t\t\t\tc.append(fn0(\n",
      "\t\t\t\t\tindex=batch_index,\n",
      "\t\t\t\t\tcorruption=self.corruption_levels[0],\n",
      "\t\t\t\t\tlr=pretrain_lr))\n",
      "\t\t\tprint 'Pre-training capa 0, epoch %d, cost %f' % (epoch, numpy.mean(c))\n",
      "\n",
      "\t\toutput = self.layer0.getOut(inputData, inputData.get_value().shape[0])\n",
      "\n",
      "\t\tout = theano.shared(numpy.asarray(output, dtype=theano.config.floatX), borrow=True)\n",
      "\n",
      "\t\tfn1 = self.layer1.pretraining(out, batch_size)\n",
      "\n",
      "\t\tfor epoch in xrange(pretraining_epochs):\n",
      "\t\t\tc = []\n",
      "\t\t\tfor batch_index in xrange(n_train_batches):\n",
      "\t\t\t\tc.append(fn1(\n",
      "\t\t\t\t\tindex=batch_index,\n",
      "\t\t\t\t\tcorruption=self.corruption_levels[1],\n",
      "\t\t\t\t\tlr=pretrain_lr))\n",
      "\t\t\tprint 'Pre-training capa 1, epoch %d, cost %f' % (epoch, numpy.mean(c))\n",
      "\n",
      "\tdef fine_tune(self, inputData, outputData, batch_size, learning_rate):\n",
      "\t\t# Indice neceario\n",
      "\t\tindex = T.iscalar('index')\n",
      "\n",
      "\t\tgparams = T.grad(self.finetune_cost, self.params)\n",
      "\t\tupdates = [(param, param - gparam * learning_rate) for param, gparam in zip(self.params, gparams)]\n",
      "\n",
      "\t\ttrain_fn = theano.function(\n",
      "\t\t\tinputs=[index],\n",
      "\t\t\toutputs=self.finetune_cost,\n",
      "\t\t\tupdates=updates,\n",
      "\t\t\tgivens={\n",
      "\t\t\t\tself.input: inputData[index * batch_size:(index + 1) * batch_size],\n",
      "\t\t\t\tself.output: outputData[index * batch_size:(index + 1) * batch_size]}, name='train')\n",
      "\n",
      "\t\treturn train_fn\n",
      "    \n",
      "    def training(self, inputData, outputData, batch_size, n_epochs, learning_rate):\n",
      "\t\tprint \"...entrenando\"\n",
      "\n",
      "\t\tn_train_batches = inputData.get_value(borrow=True).shape[0] / batch_size\n",
      "\n",
      "\t\ttrain_fn = self.fine_tune(inputData, outputData, batch_size, learning_rate)\n",
      "\n",
      "\t\tepoch = 0\n",
      "\t\tcoste = numpy.zeros((n_epochs, 1))\n",
      "\t\tepoca = []\n",
      "\t\tcoste = []\n",
      "\t\twhile (epoch < n_epochs):\n",
      "\t\t\tepoch = epoch + 1\n",
      "\t\t\tminibatch_avg_cost = 0\n",
      "\t\t\tfor minibatch_index in xrange(n_train_batches):\n",
      "\t\t\t\tminibatch_avg_cost = minibatch_avg_cost + train_fn(minibatch_index)\n",
      "\t\t\tprint 'Training MLP, epoch %d, cost %f' % (epoch, minibatch_avg_cost/n_train_batches)\n",
      "\t\t\tcoste.append(minibatch_avg_cost/n_train_batches)\n",
      "\t\t\tepoca.append(epoch)\n",
      "\t\tplt.plot(epoca, coste)\n",
      "\t\tplt.show()\n",
      "\n",
      "\tdef test(self, inputData, outputData, batch_size):\n",
      "\t\tindex = T.iscalar('index')\n",
      "\t\tn_train_batches = inputData.get_value(borrow=True).shape[0] / batch_size\n",
      "\n",
      "\t\ttest_fn = theano.function(\n",
      "\t\t\tinputs=[index],\n",
      "\t\t\toutputs=self.errors,\n",
      "\t\t\tgivens={\n",
      "\t\t\t\tself.input: inputData[index * batch_size:(index + 1) * batch_size],\n",
      "\t\t\t\tself.output: outputData[index * batch_size:(index + 1) * batch_size]}, name='test')\n",
      "\n",
      "\t\tdef test_score():\n",
      "\t\t\treturn [test_fn(i) for i in xrange(n_train_batches)]\n",
      "\t\tprint 1. - numpy.mean(test_score())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "finetune_lr = 0.1\n",
      "pretraining_epochs = 100\n",
      "corruption_levels = [.1, .2]\n",
      "pretrain_lr = 0.001\n",
      "training_epochs = 1000\n",
      "dataset = 'digits.mat'\n",
      "batch_size = 500\n",
      "\n",
      "print \"...cargando datos\"\n",
      "data = io.loadmat(dataset, squeeze_me=True)\n",
      "\n",
      "dataIn = data['X'].astype(float)\n",
      "dataOut = data['y'].astype(int)\n",
      "\n",
      "for i in range(dataOut.shape[0]):\n",
      "\tif (dataOut[i] == 10):\n",
      "\t\tdataOut[i] = 0\n",
      "\n",
      "train_set_x = theano.shared(numpy.asarray(dataIn, dtype=theano.config.floatX), borrow=True)\n",
      "train_set_y = T.cast(theano.shared(numpy.asarray(dataOut, dtype=theano.config.floatX), borrow=True), 'int32')\n",
      "\n",
      "n_train_batches = train_set_x.get_value(borrow=True).shape[0]\n",
      "n_train_batches /= batch_size\n",
      "\n",
      "x = T.matrix('x')  # Datos de entrada\n",
      "y = T.ivector('y')\t# Salida esperada\n",
      "\n",
      "sda = SdA_MLP_network(\n",
      "\tx,\n",
      "\ty,\n",
      "\tn_outs=10,\n",
      "\tn_ins=20 * 20)\n",
      "\n",
      "\n",
      "sda.pretraining(train_set_x, batch_size, pretraining_epochs, pretrain_lr)\n",
      "\n",
      "sda.training(train_set_x, train_set_y, batch_size, training_epochs, finetune_lr)\n",
      "\n",
      "sda.test(train_set_x, train_set_y, batch_size)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}