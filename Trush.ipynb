{
 "metadata": {
  "name": "",
  "signature": "sha256:e429e9332be421e93191a82f7f2a77441d48941fec845857893cdfa193c112ae"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy\n",
      "import theano\n",
      "import theano.tensor as T\n",
      "import scipy.io as io\n",
      "        \n",
      "print '... cargando datos'\n",
      "data=io.loadmat('dataLR.mat',squeeze_me=True)\n",
      "dataIn=data['data'][:,0:2]\n",
      "dataOut = data['data'][:,2]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "... cargando datos\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "Using gpu device 0: GeForce GTX 775M\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class CapaOculta(object):\n",
      "    def __init__(self, rng, input, n_in, n_out, W=None, b=None,\n",
      "                 activation=T.tanh):\n",
      "        \"\"\"\n",
      "        Capa oculta t\u00edpica de un MLP: las neuronas est\u00e1n todas conectadas y tienen una funci\u00f3n de activaci\u00f3n simoidea.\n",
      "        La matriz de pesos \"W\" es de la forma (n_in,n_out)\n",
      "        y el vector bias \"b\" (n_out,).\n",
      "\n",
      "        Nota : Usamos TANH\n",
      "\n",
      "        La funci\u00f3n de activaci\u00f3n viene dada por: tanh(dot(input,W) + b)\n",
      "\n",
      "        :type rng: numpy.random.RandomState\n",
      "        :param rng: Generador de n\u00famero aleatorios para inicializar los pesos\n",
      "\n",
      "        :type input: theano.tensor.dmatrix\n",
      "        :param input: Un tensor simb\u00f3lico para definir los datos de entrada (n_examples, n_in)\n",
      "\n",
      "        :type n_in: int\n",
      "        :param n_in: dimensionalidad de la entrada\n",
      "\n",
      "        :type n_out: int\n",
      "        :param n_out: n\u00famero de neuronas ocultas\n",
      "\n",
      "        :type activation: theano.Op or function\n",
      "        :param activation: Funci\u00f3n usada en la capa oculta\n",
      "        \"\"\"\n",
      "        self.input = input\n",
      "       \n",
      "        if W is None:\n",
      "            W_values = numpy.asarray(\n",
      "                rng.uniform(\n",
      "                    low=-numpy.sqrt(6. / (n_in + n_out)),\n",
      "                    high=numpy.sqrt(6. / (n_in + n_out)),\n",
      "                    size=(n_in, n_out)\n",
      "                ),\n",
      "                dtype=theano.config.floatX  # @UndefinedVariable\n",
      "            )\n",
      "            if activation == T.nnet.sigmoid:\n",
      "                W_values *= 4\n",
      "\n",
      "            W = theano.shared(value=W_values, name='W', borrow=True)\n",
      "\n",
      "        if b is None:\n",
      "            b_values = numpy.zeros((n_out,), dtype=theano.config.floatX)\n",
      "            b = theano.shared(value=b_values, name='b', borrow=True)\n",
      "\n",
      "        self.W = W\n",
      "        self.b = b\n",
      "\n",
      "        lin_output = T.dot(input, self.W) + self.b\n",
      "        self.output = (\n",
      "            lin_output if activation is None\n",
      "            else activation(lin_output)\n",
      "        )\n",
      "        #Par\u00e1metros del modelo\n",
      "        self.params = [self.W, self.b]\n",
      "        \n",
      "    def output(self,x):\n",
      "        return T.dot(x, self.W) + self.b\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 55
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class LogisticRegression(object):\n",
      "    \"\"\"Multi-class Logistic Regression Class\n",
      "\n",
      "    The logistic regression is fully described by a weight matrix :math:`W`\n",
      "    and bias vector :math:`b`. Classification is done by projecting data\n",
      "    points onto a set of hyperplanes, the distance to which is used to\n",
      "    determine a class membership probability.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, input, n_in, n_out):\n",
      "        \"\"\" Initialize the parameters of the logistic regression\n",
      "\n",
      "        :type input: theano.tensor.TensorType\n",
      "        :param input: symbolic variable that describes the input of the\n",
      "                      architecture (one minibatch)\n",
      "\n",
      "        :type n_in: int\n",
      "        :param n_in: number of input units, the dimension of the space in\n",
      "                     which the datapoints lie\n",
      "\n",
      "        :type n_out: int\n",
      "        :param n_out: number of output units, the dimension of the space in\n",
      "                      which the labels lie\n",
      "\n",
      "        \"\"\"\n",
      "        # start-snippet-1\n",
      "        # initialize with 0 the weights W as a matrix of shape (n_in, n_out)\n",
      "        self.W = theano.shared(\n",
      "            value=numpy.zeros(\n",
      "                (n_in, n_out),\n",
      "                dtype=theano.config.floatX\n",
      "            ),\n",
      "            name='W',\n",
      "            borrow=True\n",
      "        )\n",
      "        # initialize the baises b as a vector of n_out 0s\n",
      "        self.b = theano.shared(\n",
      "            value=numpy.zeros(\n",
      "                (n_out,),\n",
      "                dtype=theano.config.floatX\n",
      "            ),\n",
      "            name='b',\n",
      "            borrow=True\n",
      "        )\n",
      "\n",
      "        # symbolic expression for computing the matrix of class-membership\n",
      "        # probabilities\n",
      "        # Where:\n",
      "        # W is a matrix where column-k represent the separation hyper plain for\n",
      "        # class-k\n",
      "        # x is a matrix where row-j  represents input training sample-j\n",
      "        # b is a vector where element-k represent the free parameter of hyper\n",
      "        # plain-k\n",
      "        self.p_y_given_x = T.nnet.softmax(T.dot(input, self.W) + self.b)\n",
      "\n",
      "        # symbolic description of how to compute prediction as class whose\n",
      "        # probability is maximal\n",
      "        self.y_pred = T.argmax(self.p_y_given_x, axis=1)\n",
      "        # end-snippet-1\n",
      "\n",
      "        # parameters of the model\n",
      "        self.params = [self.W, self.b]\n",
      "\n",
      "    def negative_log_likelihood(self, y):\n",
      "        \"\"\"Return the mean of the negative log-likelihood of the prediction\n",
      "        of this model under a given target distribution.\n",
      "\n",
      "        .. math::\n",
      "\n",
      "            \\frac{1}{|\\mathcal{D}|} \\mathcal{L} (\\theta=\\{W,b\\}, \\mathcal{D}) =\n",
      "            \\frac{1}{|\\mathcal{D}|} \\sum_{i=0}^{|\\mathcal{D}|}\n",
      "                \\log(P(Y=y^{(i)}|x^{(i)}, W,b)) \\\\\n",
      "            \\ell (\\theta=\\{W,b\\}, \\mathcal{D})\n",
      "\n",
      "        :type y: theano.tensor.TensorType\n",
      "        :param y: corresponds to a vector that gives for each example the\n",
      "                  correct label\n",
      "\n",
      "        Note: we use the mean instead of the sum so that\n",
      "              the learning rate is less dependent on the batch size\n",
      "        \"\"\"\n",
      "        # start-snippet-2\n",
      "        # y.shape[0] is (symbolically) the number of rows in y, i.e.,\n",
      "        # number of examples (call it n) in the minibatch\n",
      "        # T.arange(y.shape[0]) is a symbolic vector which will contain\n",
      "        # [0,1,2,... n-1] T.log(self.p_y_given_x) is a matrix of\n",
      "        # Log-Probabilities (call it LP) with one row per example and\n",
      "        # one column per class LP[T.arange(y.shape[0]),y] is a vector\n",
      "        # v containing [LP[0,y[0]], LP[1,y[1]], LP[2,y[2]], ...,\n",
      "        # LP[n-1,y[n-1]]] and T.mean(LP[T.arange(y.shape[0]),y]) is\n",
      "        # the mean (across minibatch examples) of the elements in v,\n",
      "        # i.e., the mean log-likelihood across the minibatch.\n",
      "        return -T.mean(T.log(self.p_y_given_x)[T.arange(y.shape[0]), y])\n",
      "        # end-snippet-2\n",
      "\n",
      "    def errors(self, y):\n",
      "        \"\"\"Return a float representing the number of errors in the minibatch\n",
      "        over the total number of examples of the minibatch ; zero one\n",
      "        loss over the size of the minibatch\n",
      "\n",
      "        :type y: theano.tensor.TensorType\n",
      "        :param y: corresponds to a vector that gives for each example the\n",
      "                  correct label\n",
      "        \"\"\"\n",
      "\n",
      "        # check if y has same dimension of y_pred\n",
      "        if y.ndim != self.y_pred.ndim:\n",
      "            raise TypeError(\n",
      "                'y should have the same shape as self.y_pred',\n",
      "                ('y', y.type, 'y_pred', self.y_pred.type)\n",
      "            )\n",
      "        # check if y is of the correct datatype\n",
      "        if y.dtype.startswith('int'):\n",
      "            # the T.neq operator returns a vector of 0s and 1s, where 1\n",
      "            # represents a mistake in prediction\n",
      "            return T.mean(T.neq(self.y_pred, y))\n",
      "        else:\n",
      "            raise NotImplementedError()\n",
      "        \n",
      "    def output(self,x):\n",
      "        pred=T.nnet.softmax(T.dot(x, self.W) + self.b)\n",
      "        return T.argmax(pred, axis=1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 56
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class MLP(object):\n",
      "    \"\"\"Clase Perceptr\u00f3n multicapa\n",
      "\n",
      "    Vamos a definir una sola capa oculta usando la clase CapaOculta que hemos creado anteriormente, y usaremos una capa de\n",
      "    salida tipo softmax para la que usaremos la clase LogisticRegression.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, rng, input, n_in, n_hidden, n_out):\n",
      "        \"\"\"Initialize the parameters for the multilayer perceptron\n",
      "\n",
      "        :type rng: numpy.random.RandomState\n",
      "        :param rng: Generador de n\u00famero aleatorios para la inicializaci\u00f3n de los pesos\n",
      "\n",
      "        :type input: theano.tensor.TensorType\n",
      "        :param input: Variable simb\u00f3lica para la entrada al MLP\n",
      "\n",
      "        :type n_in: int\n",
      "        :param n_in: N\u00famero de neuronas de entrada\n",
      "\n",
      "        :type n_hidden: int\n",
      "        :param n_hidden: N\u00famero de neuronas en la capa oculta\n",
      "\n",
      "        :type n_out: int\n",
      "        :param n_out: N\u00famero de neuronas en la capa de salida\n",
      "\n",
      "        \"\"\"\n",
      "\n",
      "        # Creamos la capa oculta\n",
      "        self.hiddenLayer = CapaOculta(\n",
      "            rng=rng,\n",
      "            input=input,\n",
      "            n_in=n_in,\n",
      "            n_out=n_hidden,\n",
      "            activation=T.tanh\n",
      "        )\n",
      "\n",
      "        # La capa LR tendr\u00e1 como entrada las neuronas de la capa oculta\n",
      "        self.logRegressionLayer = LogisticRegression(\n",
      "            input=self.hiddenLayer.output,\n",
      "            n_in=n_hidden,\n",
      "            n_out=n_out\n",
      "        )\n",
      "        # end-snippet-2 start-snippet-3\n",
      "        # L1 norm: Nos sirve para regularizar.\n",
      "        self.L1 = (\n",
      "            abs(self.hiddenLayer.W).sum()\n",
      "            + abs(self.logRegressionLayer.W).sum()\n",
      "        )\n",
      "\n",
      "        # square of L2 norm: otra forma de regularizar.\n",
      "        self.L2_sqr = (\n",
      "            (self.hiddenLayer.W ** 2).sum()\n",
      "            + (self.logRegressionLayer.W ** 2).sum()\n",
      "        )\n",
      "\n",
      "        # Return the mean of the negative log-likelihood of the prediction\n",
      "        # of this model under a given target distribution.\n",
      "        self.negative_log_likelihood = (\n",
      "            self.logRegressionLayer.negative_log_likelihood\n",
      "        )\n",
      "        # Almacenamos los errores\n",
      "        self.errors = self.logRegressionLayer.errors\n",
      "\n",
      "        # Guardamos como par\u00e1metros los par\u00e1metros de las dos capas\n",
      "        self.params = self.hiddenLayer.params + self.logRegressionLayer.params\n",
      "        \n",
      "    def output(self,x):\n",
      "        #respuestaHidden = self.hiddenLayer.output()\n",
      "        return self.logRegressionLayer.output(x)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 59
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def test_mlp(learning_rate=0.01, L1_reg=0.00, L2_reg=0.0001, n_epochs=1000,\n",
      "             batch_size=20, n_hidden=30):\n",
      "    train_set_x = theano.shared(numpy.asarray(dataIn,\n",
      "                    dtype=theano.config.floatX),borrow=True)\n",
      "    train_set_y = T.cast(theano.shared(numpy.asarray(dataOut,\n",
      "                    dtype=theano.config.floatX),borrow=True),'int32')\n",
      "\n",
      "    n_train_batches = train_set_x.get_value(borrow=True).shape[0] / batch_size\n",
      "    \n",
      "    print '... building the model'\n",
      "    \n",
      "    index = T.lscalar()  # \u00cdndice del lote\n",
      "    x = T.matrix('x')  # Datos de entrada\n",
      "    y = T.ivector('y')  # Datos de salida esperados\n",
      "\n",
      "    rng = numpy.random.RandomState(1234)\n",
      "\n",
      "    # Construimos el objeto MLP\n",
      "    classifier = MLP(\n",
      "        rng=rng,\n",
      "        input=x,\n",
      "        n_in=2,\n",
      "        n_hidden=n_hidden,\n",
      "        n_out=2\n",
      "    )\n",
      "    \n",
      "    # Funci\u00f3n de coste a minimizar\n",
      "    cost = (\n",
      "        classifier.negative_log_likelihood(y)\n",
      "        + L1_reg * classifier.L1\n",
      "        + L2_reg * classifier.L2_sqr\n",
      "    )\n",
      "    \n",
      "    # Calculamos el gradiente de la funci\u00f3n de coste con respecto a los par\u00e1metros de las dos capas\n",
      "    gparams = [T.grad(cost, param) for param in classifier.params]\n",
      "\n",
      "    # Y definimos las actualizaciones de los par\u00e1metros\n",
      "    updates = [\n",
      "        (param, param - learning_rate * gparam)\n",
      "        for param, gparam in zip(classifier.params, gparams)\n",
      "    ]\n",
      "    \n",
      "    \n",
      "    # Compilamos la funci\u00f3n de aprendizaje\n",
      "    train_model = theano.function(\n",
      "        inputs=[index],\n",
      "        outputs=cost,\n",
      "        updates=updates,\n",
      "        givens={\n",
      "            x: train_set_x[index * batch_size: (index + 1) * batch_size],\n",
      "            y: train_set_y[index * batch_size: (index + 1) * batch_size]\n",
      "        }\n",
      "    )\n",
      "    \n",
      "    # Entrenamos la red\n",
      "    print '... entrenando'\n",
      "    epoch = 0\n",
      "    while (epoch < n_epochs):\n",
      "        epoch = epoch + 1\n",
      "        minibatch_avg_cost = 0\n",
      "        for minibatch_index in xrange(n_train_batches):\n",
      "            minibatch_avg_cost = minibatch_avg_cost + train_model(minibatch_index)\n",
      "        #print \"\u00c9poca: \" + repr(epoch) + \" - Error medio: \" + repr(minibatch_avg_cost/n_train_batches)\n",
      "      \n",
      "    # Sacamos los resultados por pantalla\n",
      "    print dataOut\n",
      "    for i in range(dataIn.shape[0]):\n",
      "        print classifier.output(dataIn[i])\n",
      "    \n",
      "if __name__ == '__main__':\n",
      "    test_mlp()    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "... building the model\n",
        "... entrenando"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[ 0.  0.  0.  1.  1.  0.  1.  1.  1.  1.  0.  0.  1.  1.  0.  1.  1.  0.\n",
        "  1.  1.  0.  1.  0.  0.  1.  1.  1.  0.  0.  0.  1.  1.  0.  1.  0.  0.\n",
        "  0.  1.  0.  0.  1.  0.  1.  0.  0.  0.  1.  1.  1.  1.  1.  1.  1.  0.\n",
        "  0.  0.  1.  0.  1.  1.  1.  0.  0.  0.  0.  0.  1.  0.  1.  1.  0.  1.\n",
        "  1.  1.  1.  1.  1.  1.  0.  0.  1.  1.  1.  1.  1.  1.  0.  1.  1.  0.\n",
        "  1.  1.  0.  1.  1.  1.  1.  1.  1.  1.]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n"
       ]
      }
     ],
     "prompt_number": 60
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y_pred"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'y_pred' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-14-5a040a10fabc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;31mNameError\u001b[0m: name 'y_pred' is not defined"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import theano.tensor as T\n",
      "from theano import function\n",
      "\n",
      "x = T.dmatrix('x')\n",
      "s = 1 / (1 + T.exp(-x))\n",
      "logistic = function([x], s)\n",
      "logistic([[0, 1], [-1, -2]])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 3,
       "text": [
        "array([[ 0.5       ,  0.73105858],\n",
        "       [ 0.26894142,  0.11920292]])"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import scipy.io as io\n",
      "        \n",
      "print '... cargando datos'\n",
      "data=io.loadmat('dataLR.mat',squeeze_me=True)\n",
      "dataIn=data['data'][:,0:2].astype(float)\n",
      "dataOut = data['data'][:,2].astype(int)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "... cargando datos\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "\n",
      "x=np.zeros((100,2))\n",
      "y=np.zeros((100,2))\n",
      "for i in range(100):\n",
      "    if (dataOut[i]==1):\n",
      "        x[i,:]=dataIn[i,:]\n",
      "    else:\n",
      "        y[i,:]=dataIn[i,:]\n",
      "\n",
      "plt.plot(x[:,0],x[:,1],'ro', y[:,0],y[:,1],'go')\n",
      "plt.axis([25,100,25,100])\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy\n",
      "import theano\n",
      "import theano.tensor as T\n",
      "rng = numpy.random\n",
      "\n",
      "steps=100000\n",
      "feats=2\n",
      "x = T.lmatrix(\"x\")\n",
      "y = T.lvector(\"y\")\n",
      "w = theano.shared(rng.randn(feats))\n",
      "b = theano.shared(0.)\n",
      "print \"Modelo inicial:\"\n",
      "print \"W (tama\u00f1o): \" + repr(w.get_value().shape)\n",
      "print \"b (valor): \" + repr(b.get_value())\n",
      "\n",
      "import scipy.io as io\n",
      "        \n",
      "print '... cargando datos'\n",
      "data=io.loadmat('dataLR.mat',squeeze_me=True)\n",
      "dataIn=data['data'][:,0:2].astype(float)\n",
      "dataOut = data['data'][:,2].astype(int)\n",
      "\n",
      "'''N = 400\n",
      "feats = 2\n",
      "D = (rng.randn(N, feats), rng.randint(size=N, low=0, high=2))\n",
      "#dataIn=D[0]\n",
      "#dataOut=D[1]\n",
      "'''\n",
      "\n",
      "# Construct Theano expression graph\n",
      "p_1 = 1 / (1 + T.exp(-T.dot(x, w) - b))   # Probability that target = 1\n",
      "prediction = p_1 > 0.5                    # The prediction thresholded\n",
      "xent = -y * T.log(p_1) - (1-y) * T.log(1-p_1) # Cross-entropy loss function\n",
      "cost = xent.mean() + 0.01 * (w ** 2).sum()# The cost to minimize\n",
      "gw, gb = T.grad(cost, [w, b])             # Compute the gradient of the cost\n",
      "                                          # (we shall return to this in a\n",
      "                                          # following section of this tutorial\n",
      "        \n",
      "# Compile\n",
      "train = theano.function(\n",
      "          inputs=[x,y],\n",
      "          outputs=[prediction, xent],\n",
      "          updates=((w, w - 0.1 * gw), (b, b - 0.1 * gb)),allow_input_downcast=True)\n",
      "predict = theano.function(inputs=[x], outputs=prediction,allow_input_downcast=True)\n",
      "\n",
      "# Train\n",
      "for i in range(steps):\n",
      "    pred, err = train(dataIn, dataOut)\n",
      "    \n",
      "print \"Valores esperados: \", dataOut\n",
      "pp= predict(dataIn)\n",
      "print \"Valores previstos: \", pp\n",
      "\n",
      "print \"Tasa de acierto: \", map(lambda x,y:x==y, dataOut, predict(dataIn)).count(True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Modelo inicial:\n",
        "W (tama\u00f1o): (2,)\n",
        "b (valor): array(0.0)\n",
        "... cargando datos\n",
        "Valores esperados: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " [0 0 0 1 1 0 1 1 1 1 0 0 1 1 0 1 1 0 1 1 0 1 0 0 1 1 1 0 0 0 1 1 0 1 0 0 0\n",
        " 1 0 0 1 0 1 0 0 0 1 1 1 1 1 1 1 0 0 0 1 0 1 1 1 0 0 0 0 0 1 0 1 1 0 1 1 1\n",
        " 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1]\n",
        "Valores previstos:  [1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
        " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 0 1 1 1\n",
        " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
        "Tasa de acierto:  65\n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "\n",
      "x=np.zeros((100,2))\n",
      "y=np.zeros((100,2))\n",
      "for i in range(100):\n",
      "    if (pp[i]==1):\n",
      "        x[i,:]=dataIn[i,:]\n",
      "    else:\n",
      "        y[i,:]=dataIn[i,:]\n",
      "\n",
      "plt.plot(x[:,0],x[:,1],'ro', y[:,0],y[:,1],'go')\n",
      "plt.axis([25,100,25,100])\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}