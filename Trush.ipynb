{
 "metadata": {
  "name": "",
  "signature": "sha256:7182291cd44f4dbfb6022593bbd3d5476a8954f28683a057d0a2f0a126a7e01a"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy\n",
      "import theano\n",
      "import theano.tensor as T\n",
      "import scipy.io as io\n",
      "        \n",
      "print '... cargando datos'\n",
      "data=io.loadmat('dataLR.mat',squeeze_me=True)\n",
      "dataIn=data['data'][:,0:2]\n",
      "dataOut = data['data'][:,2]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "Using gpu device 0: GeForce GTX 775M\n"
       ]
      },
      {
       "ename": "OSError",
       "evalue": "[Errno 13] Permission denied: '/Users/jelias/.theano/compiledir_Darwin-14.0.0-x86_64-i386-64bit-i386-2.7.9-64/tmp09vYZ7'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-1-d7b10cec8796>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/jelias/anaconda/lib/python2.7/site-packages/theano/__init__.pyc\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msandbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_driver\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msandbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_driver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_nvidia_driver1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'opencl'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/jelias/anaconda/lib/python2.7/site-packages/theano/sandbox/cuda/tests/test_driver.pyc\u001b[0m in \u001b[0;36mtest_nvidia_driver1\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshared_constructor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     f = theano.function(inputs=[], outputs=A.sum(), mode=mode_with_gpu,\n\u001b[0;32m---> 32\u001b[0;31m                         profile=False)\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0mtopo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoposort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopo\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/jelias/anaconda/lib/python2.7/site-packages/theano/compile/function.pyc\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(inputs, outputs, mode, updates, givens, no_default_updates, accept_inplace, name, rebuild_strict, allow_input_downcast, profile, on_unused_input)\u001b[0m\n\u001b[1;32m    264\u001b[0m                 \u001b[0mallow_input_downcast\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_input_downcast\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m                 \u001b[0mon_unused_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mon_unused_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m                 profile=profile)\n\u001b[0m\u001b[1;32m    267\u001b[0m     \u001b[0;31m# We need to add the flag check_aliased inputs if we have any mutable or\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;31m# borrowed used defined inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/jelias/anaconda/lib/python2.7/site-packages/theano/compile/pfunc.pyc\u001b[0m in \u001b[0;36mpfunc\u001b[0;34m(params, outputs, mode, updates, givens, no_default_updates, accept_inplace, name, rebuild_strict, allow_input_downcast, profile, on_unused_input)\u001b[0m\n\u001b[1;32m    509\u001b[0m     return orig_function(inputs, cloned_outputs, mode,\n\u001b[1;32m    510\u001b[0m             \u001b[0maccept_inplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_inplace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprofile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m             on_unused_input=on_unused_input)\n\u001b[0m\u001b[1;32m    512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/jelias/anaconda/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36morig_function\u001b[0;34m(inputs, outputs, mode, accept_inplace, name, profile, on_unused_input)\u001b[0m\n\u001b[1;32m   1465\u001b[0m                    \u001b[0maccept_inplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_inplace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1466\u001b[0m                    \u001b[0mprofile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1467\u001b[0;31m                    \u001b[0mon_unused_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mon_unused_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1468\u001b[0m                        defaults)\n\u001b[1;32m   1469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/jelias/anaconda/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, inputs, outputs, mode, accept_inplace, function_builder, profile, on_unused_input, fgraph)\u001b[0m\n\u001b[1;32m   1103\u001b[0m             \u001b[0;31m# 3) This helps propagate knowledge of newly compiled modules to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m             \u001b[0;31m#    concurrent processes.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1105\u001b[0;31m             \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgof\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_module_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1106\u001b[0m         \u001b[0;31m# Handle the case where inputs and/or outputs is a single\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1107\u001b[0m         \u001b[0;31m# Variable (not in a list)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/jelias/anaconda/lib/python2.7/site-packages/theano/gof/cc.pyc\u001b[0m in \u001b[0;36mget_module_cache\u001b[0;34m(init_args)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0mbe\u001b[0m \u001b[0mforwarded\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mModuleCache\u001b[0m \u001b[0mconstructor\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mkeyword\u001b[0m \u001b[0marguments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \"\"\"\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_module_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompiledir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minit_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/jelias/anaconda/lib/python2.7/site-packages/theano/gof/cmodule.pyc\u001b[0m in \u001b[0;36mget_module_cache\u001b[0;34m(dirname, init_args)\u001b[0m\n\u001b[1;32m   1387\u001b[0m         \u001b[0minit_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1388\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_module_cache\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1389\u001b[0;31m         \u001b[0m_module_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModuleCache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0minit_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m         \u001b[0matexit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_module_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_on_atexit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0minit_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/jelias/anaconda/lib/python2.7/site-packages/theano/gof/cmodule.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dirname, check_for_broken_eq, do_refresh)\u001b[0m\n\u001b[1;32m    614\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdo_refresh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 616\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    617\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    618\u001b[0m     \u001b[0mage_thresh_use\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m60\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m60\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m24\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m24\u001b[0m    \u001b[0;31m# 24 days\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/jelias/anaconda/lib/python2.7/site-packages/theano/gof/cmodule.pyc\u001b[0m in \u001b[0;36mrefresh\u001b[0;34m(self, age_thresh_use, delete_if_problem, cleanup)\u001b[0m\n\u001b[1;32m    689\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 691\u001b[0;31m             \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    692\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m                 rmtree_empty(root, ignore_nocleanup=True,\n",
        "\u001b[0;31mOSError\u001b[0m: [Errno 13] Permission denied: '/Users/jelias/.theano/compiledir_Darwin-14.0.0-x86_64-i386-64bit-i386-2.7.9-64/tmp09vYZ7'"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class CapaOculta(object):\n",
      "    def __init__(self, rng, input, n_in, n_out, W=None, b=None,\n",
      "                 activation=T.tanh):\n",
      "        \"\"\"\n",
      "        Capa oculta t\u00edpica de un MLP: las neuronas est\u00e1n todas conectadas y tienen una funci\u00f3n de activaci\u00f3n simoidea.\n",
      "        La matriz de pesos \"W\" es de la forma (n_in,n_out)\n",
      "        y el vector bias \"b\" (n_out,).\n",
      "\n",
      "        Nota : Usamos TANH\n",
      "\n",
      "        La funci\u00f3n de activaci\u00f3n viene dada por: tanh(dot(input,W) + b)\n",
      "\n",
      "        :type rng: numpy.random.RandomState\n",
      "        :param rng: Generador de n\u00famero aleatorios para inicializar los pesos\n",
      "\n",
      "        :type input: theano.tensor.dmatrix\n",
      "        :param input: Un tensor simb\u00f3lico para definir los datos de entrada (n_examples, n_in)\n",
      "\n",
      "        :type n_in: int\n",
      "        :param n_in: dimensionalidad de la entrada\n",
      "\n",
      "        :type n_out: int\n",
      "        :param n_out: n\u00famero de neuronas ocultas\n",
      "\n",
      "        :type activation: theano.Op or function\n",
      "        :param activation: Funci\u00f3n usada en la capa oculta\n",
      "        \"\"\"\n",
      "        self.input = input\n",
      "       \n",
      "        if W is None:\n",
      "            W_values = numpy.asarray(\n",
      "                rng.uniform(\n",
      "                    low=-numpy.sqrt(6. / (n_in + n_out)),\n",
      "                    high=numpy.sqrt(6. / (n_in + n_out)),\n",
      "                    size=(n_in, n_out)\n",
      "                ),\n",
      "                dtype=theano.config.floatX  # @UndefinedVariable\n",
      "            )\n",
      "            if activation == T.nnet.sigmoid:\n",
      "                W_values *= 4\n",
      "\n",
      "            W = theano.shared(value=W_values, name='W', borrow=True)\n",
      "\n",
      "        if b is None:\n",
      "            b_values = numpy.zeros((n_out,), dtype=theano.config.floatX)\n",
      "            b = theano.shared(value=b_values, name='b', borrow=True)\n",
      "\n",
      "        self.W = W\n",
      "        self.b = b\n",
      "\n",
      "        lin_output = T.dot(input, self.W) + self.b\n",
      "        self.output = (\n",
      "            lin_output if activation is None\n",
      "            else activation(lin_output)\n",
      "        )\n",
      "        #Par\u00e1metros del modelo\n",
      "        self.params = [self.W, self.b]\n",
      "        \n",
      "    def output(self,x):\n",
      "        return T.dot(x, self.W) + self.b\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'T' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-2-ffb9fef2ce1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mCapaOculta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     def __init__(self, rng, input, n_in, n_out, W=None, b=None,\n\u001b[1;32m      3\u001b[0m                  activation=T.tanh):\n\u001b[1;32m      4\u001b[0m         \"\"\"\n\u001b[1;32m      5\u001b[0m         \u001b[0mCapa\u001b[0m \u001b[0moculta\u001b[0m \u001b[0mt\u001b[0m\u001b[0;31m\u00ed\u001b[0m\u001b[0mpica\u001b[0m \u001b[0mde\u001b[0m \u001b[0mun\u001b[0m \u001b[0mMLP\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlas\u001b[0m \u001b[0mneuronas\u001b[0m \u001b[0mest\u001b[0m\u001b[0;31m\u00e1\u001b[0m\u001b[0mn\u001b[0m \u001b[0mtodas\u001b[0m \u001b[0mconectadas\u001b[0m \u001b[0my\u001b[0m \u001b[0mtienen\u001b[0m \u001b[0muna\u001b[0m \u001b[0mfunci\u001b[0m\u001b[0;31m\u00f3\u001b[0m\u001b[0mn\u001b[0m \u001b[0mde\u001b[0m \u001b[0mactivaci\u001b[0m\u001b[0;31m\u00f3\u001b[0m\u001b[0mn\u001b[0m \u001b[0msimoidea\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m<ipython-input-2-ffb9fef2ce1d>\u001b[0m in \u001b[0;36mCapaOculta\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mCapaOculta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     def __init__(self, rng, input, n_in, n_out, W=None, b=None,\n\u001b[0;32m----> 3\u001b[0;31m                  activation=T.tanh):\n\u001b[0m\u001b[1;32m      4\u001b[0m         \"\"\"\n\u001b[1;32m      5\u001b[0m         \u001b[0mCapa\u001b[0m \u001b[0moculta\u001b[0m \u001b[0mt\u001b[0m\u001b[0;31m\u00ed\u001b[0m\u001b[0mpica\u001b[0m \u001b[0mde\u001b[0m \u001b[0mun\u001b[0m \u001b[0mMLP\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlas\u001b[0m \u001b[0mneuronas\u001b[0m \u001b[0mest\u001b[0m\u001b[0;31m\u00e1\u001b[0m\u001b[0mn\u001b[0m \u001b[0mtodas\u001b[0m \u001b[0mconectadas\u001b[0m \u001b[0my\u001b[0m \u001b[0mtienen\u001b[0m \u001b[0muna\u001b[0m \u001b[0mfunci\u001b[0m\u001b[0;31m\u00f3\u001b[0m\u001b[0mn\u001b[0m \u001b[0mde\u001b[0m \u001b[0mactivaci\u001b[0m\u001b[0;31m\u00f3\u001b[0m\u001b[0mn\u001b[0m \u001b[0msimoidea\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mNameError\u001b[0m: name 'T' is not defined"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class LogisticRegression(object):\n",
      "    \"\"\"Multi-class Logistic Regression Class\n",
      "\n",
      "    The logistic regression is fully described by a weight matrix :math:`W`\n",
      "    and bias vector :math:`b`. Classification is done by projecting data\n",
      "    points onto a set of hyperplanes, the distance to which is used to\n",
      "    determine a class membership probability.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, input, n_in, n_out):\n",
      "        \"\"\" Initialize the parameters of the logistic regression\n",
      "\n",
      "        :type input: theano.tensor.TensorType\n",
      "        :param input: symbolic variable that describes the input of the\n",
      "                      architecture (one minibatch)\n",
      "\n",
      "        :type n_in: int\n",
      "        :param n_in: number of input units, the dimension of the space in\n",
      "                     which the datapoints lie\n",
      "\n",
      "        :type n_out: int\n",
      "        :param n_out: number of output units, the dimension of the space in\n",
      "                      which the labels lie\n",
      "\n",
      "        \"\"\"\n",
      "        # start-snippet-1\n",
      "        # initialize with 0 the weights W as a matrix of shape (n_in, n_out)\n",
      "        self.W = theano.shared(\n",
      "            value=numpy.zeros(\n",
      "                (n_in, n_out),\n",
      "                dtype=theano.config.floatX\n",
      "            ),\n",
      "            name='W',\n",
      "            borrow=True\n",
      "        )\n",
      "        # initialize the baises b as a vector of n_out 0s\n",
      "        self.b = theano.shared(\n",
      "            value=numpy.zeros(\n",
      "                (n_out,),\n",
      "                dtype=theano.config.floatX\n",
      "            ),\n",
      "            name='b',\n",
      "            borrow=True\n",
      "        )\n",
      "\n",
      "        # symbolic expression for computing the matrix of class-membership\n",
      "        # probabilities\n",
      "        # Where:\n",
      "        # W is a matrix where column-k represent the separation hyper plain for\n",
      "        # class-k\n",
      "        # x is a matrix where row-j  represents input training sample-j\n",
      "        # b is a vector where element-k represent the free parameter of hyper\n",
      "        # plain-k\n",
      "        self.p_y_given_x = T.nnet.softmax(T.dot(input, self.W) + self.b)\n",
      "\n",
      "        # symbolic description of how to compute prediction as class whose\n",
      "        # probability is maximal\n",
      "        self.y_pred = T.argmax(self.p_y_given_x, axis=1)\n",
      "        # end-snippet-1\n",
      "\n",
      "        # parameters of the model\n",
      "        self.params = [self.W, self.b]\n",
      "\n",
      "    def negative_log_likelihood(self, y):\n",
      "        \"\"\"Return the mean of the negative log-likelihood of the prediction\n",
      "        of this model under a given target distribution.\n",
      "\n",
      "        .. math::\n",
      "\n",
      "            \\frac{1}{|\\mathcal{D}|} \\mathcal{L} (\\theta=\\{W,b\\}, \\mathcal{D}) =\n",
      "            \\frac{1}{|\\mathcal{D}|} \\sum_{i=0}^{|\\mathcal{D}|}\n",
      "                \\log(P(Y=y^{(i)}|x^{(i)}, W,b)) \\\\\n",
      "            \\ell (\\theta=\\{W,b\\}, \\mathcal{D})\n",
      "\n",
      "        :type y: theano.tensor.TensorType\n",
      "        :param y: corresponds to a vector that gives for each example the\n",
      "                  correct label\n",
      "\n",
      "        Note: we use the mean instead of the sum so that\n",
      "              the learning rate is less dependent on the batch size\n",
      "        \"\"\"\n",
      "        # start-snippet-2\n",
      "        # y.shape[0] is (symbolically) the number of rows in y, i.e.,\n",
      "        # number of examples (call it n) in the minibatch\n",
      "        # T.arange(y.shape[0]) is a symbolic vector which will contain\n",
      "        # [0,1,2,... n-1] T.log(self.p_y_given_x) is a matrix of\n",
      "        # Log-Probabilities (call it LP) with one row per example and\n",
      "        # one column per class LP[T.arange(y.shape[0]),y] is a vector\n",
      "        # v containing [LP[0,y[0]], LP[1,y[1]], LP[2,y[2]], ...,\n",
      "        # LP[n-1,y[n-1]]] and T.mean(LP[T.arange(y.shape[0]),y]) is\n",
      "        # the mean (across minibatch examples) of the elements in v,\n",
      "        # i.e., the mean log-likelihood across the minibatch.\n",
      "        return -T.mean(T.log(self.p_y_given_x)[T.arange(y.shape[0]), y])\n",
      "        # end-snippet-2\n",
      "\n",
      "    def errors(self, y):\n",
      "        \"\"\"Return a float representing the number of errors in the minibatch\n",
      "        over the total number of examples of the minibatch ; zero one\n",
      "        loss over the size of the minibatch\n",
      "\n",
      "        :type y: theano.tensor.TensorType\n",
      "        :param y: corresponds to a vector that gives for each example the\n",
      "                  correct label\n",
      "        \"\"\"\n",
      "\n",
      "        # check if y has same dimension of y_pred\n",
      "        if y.ndim != self.y_pred.ndim:\n",
      "            raise TypeError(\n",
      "                'y should have the same shape as self.y_pred',\n",
      "                ('y', y.type, 'y_pred', self.y_pred.type)\n",
      "            )\n",
      "        # check if y is of the correct datatype\n",
      "        if y.dtype.startswith('int'):\n",
      "            # the T.neq operator returns a vector of 0s and 1s, where 1\n",
      "            # represents a mistake in prediction\n",
      "            return T.mean(T.neq(self.y_pred, y))\n",
      "        else:\n",
      "            raise NotImplementedError()\n",
      "        \n",
      "    def output(self,x):\n",
      "        pred=T.nnet.softmax(T.dot(x, self.W) + self.b)\n",
      "        return T.argmax(pred, axis=1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 56
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class MLP(object):\n",
      "    \"\"\"Clase Perceptr\u00f3n multicapa\n",
      "\n",
      "    Vamos a definir una sola capa oculta usando la clase CapaOculta que hemos creado anteriormente, y usaremos una capa de\n",
      "    salida tipo softmax para la que usaremos la clase LogisticRegression.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, rng, input, n_in, n_hidden, n_out):\n",
      "        \"\"\"Initialize the parameters for the multilayer perceptron\n",
      "\n",
      "        :type rng: numpy.random.RandomState\n",
      "        :param rng: Generador de n\u00famero aleatorios para la inicializaci\u00f3n de los pesos\n",
      "\n",
      "        :type input: theano.tensor.TensorType\n",
      "        :param input: Variable simb\u00f3lica para la entrada al MLP\n",
      "\n",
      "        :type n_in: int\n",
      "        :param n_in: N\u00famero de neuronas de entrada\n",
      "\n",
      "        :type n_hidden: int\n",
      "        :param n_hidden: N\u00famero de neuronas en la capa oculta\n",
      "\n",
      "        :type n_out: int\n",
      "        :param n_out: N\u00famero de neuronas en la capa de salida\n",
      "\n",
      "        \"\"\"\n",
      "\n",
      "        # Creamos la capa oculta\n",
      "        self.hiddenLayer = CapaOculta(\n",
      "            rng=rng,\n",
      "            input=input,\n",
      "            n_in=n_in,\n",
      "            n_out=n_hidden,\n",
      "            activation=T.tanh\n",
      "        )\n",
      "\n",
      "        # La capa LR tendr\u00e1 como entrada las neuronas de la capa oculta\n",
      "        self.logRegressionLayer = LogisticRegression(\n",
      "            input=self.hiddenLayer.output,\n",
      "            n_in=n_hidden,\n",
      "            n_out=n_out\n",
      "        )\n",
      "        # end-snippet-2 start-snippet-3\n",
      "        # L1 norm: Nos sirve para regularizar.\n",
      "        self.L1 = (\n",
      "            abs(self.hiddenLayer.W).sum()\n",
      "            + abs(self.logRegressionLayer.W).sum()\n",
      "        )\n",
      "\n",
      "        # square of L2 norm: otra forma de regularizar.\n",
      "        self.L2_sqr = (\n",
      "            (self.hiddenLayer.W ** 2).sum()\n",
      "            + (self.logRegressionLayer.W ** 2).sum()\n",
      "        )\n",
      "\n",
      "        # Return the mean of the negative log-likelihood of the prediction\n",
      "        # of this model under a given target distribution.\n",
      "        self.negative_log_likelihood = (\n",
      "            self.logRegressionLayer.negative_log_likelihood\n",
      "        )\n",
      "        # Almacenamos los errores\n",
      "        self.errors = self.logRegressionLayer.errors\n",
      "\n",
      "        # Guardamos como par\u00e1metros los par\u00e1metros de las dos capas\n",
      "        self.params = self.hiddenLayer.params + self.logRegressionLayer.params\n",
      "        \n",
      "    def output(self,x):\n",
      "        #respuestaHidden = self.hiddenLayer.output()\n",
      "        return self.logRegressionLayer.output(x)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 59
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def test_mlp(learning_rate=0.01, L1_reg=0.00, L2_reg=0.0001, n_epochs=1000,\n",
      "             batch_size=20, n_hidden=30):\n",
      "    train_set_x = theano.shared(numpy.asarray(dataIn,\n",
      "                    dtype=theano.config.floatX),borrow=True)\n",
      "    train_set_y = T.cast(theano.shared(numpy.asarray(dataOut,\n",
      "                    dtype=theano.config.floatX),borrow=True),'int32')\n",
      "\n",
      "    n_train_batches = train_set_x.get_value(borrow=True).shape[0] / batch_size\n",
      "    \n",
      "    print '... building the model'\n",
      "    \n",
      "    index = T.lscalar()  # \u00cdndice del lote\n",
      "    x = T.matrix('x')  # Datos de entrada\n",
      "    y = T.ivector('y')  # Datos de salida esperados\n",
      "\n",
      "    rng = numpy.random.RandomState(1234)\n",
      "\n",
      "    # Construimos el objeto MLP\n",
      "    classifier = MLP(\n",
      "        rng=rng,\n",
      "        input=x,\n",
      "        n_in=2,\n",
      "        n_hidden=n_hidden,\n",
      "        n_out=2\n",
      "    )\n",
      "    \n",
      "    # Funci\u00f3n de coste a minimizar\n",
      "    cost = (\n",
      "        classifier.negative_log_likelihood(y)\n",
      "        + L1_reg * classifier.L1\n",
      "        + L2_reg * classifier.L2_sqr\n",
      "    )\n",
      "    \n",
      "    # Calculamos el gradiente de la funci\u00f3n de coste con respecto a los par\u00e1metros de las dos capas\n",
      "    gparams = [T.grad(cost, param) for param in classifier.params]\n",
      "\n",
      "    # Y definimos las actualizaciones de los par\u00e1metros\n",
      "    updates = [\n",
      "        (param, param - learning_rate * gparam)\n",
      "        for param, gparam in zip(classifier.params, gparams)\n",
      "    ]\n",
      "    \n",
      "    \n",
      "    # Compilamos la funci\u00f3n de aprendizaje\n",
      "    train_model = theano.function(\n",
      "        inputs=[index],\n",
      "        outputs=cost,\n",
      "        updates=updates,\n",
      "        givens={\n",
      "            x: train_set_x[index * batch_size: (index + 1) * batch_size],\n",
      "            y: train_set_y[index * batch_size: (index + 1) * batch_size]\n",
      "        }\n",
      "    )\n",
      "    \n",
      "    # Entrenamos la red\n",
      "    print '... entrenando'\n",
      "    epoch = 0\n",
      "    while (epoch < n_epochs):\n",
      "        epoch = epoch + 1\n",
      "        minibatch_avg_cost = 0\n",
      "        for minibatch_index in xrange(n_train_batches):\n",
      "            minibatch_avg_cost = minibatch_avg_cost + train_model(minibatch_index)\n",
      "        #print \"\u00c9poca: \" + repr(epoch) + \" - Error medio: \" + repr(minibatch_avg_cost/n_train_batches)\n",
      "      \n",
      "    # Sacamos los resultados por pantalla\n",
      "    print dataOut\n",
      "    for i in range(dataIn.shape[0]):\n",
      "        print classifier.output(dataIn[i])\n",
      "    \n",
      "if __name__ == '__main__':\n",
      "    test_mlp()    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "... building the model\n",
        "... entrenando"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[ 0.  0.  0.  1.  1.  0.  1.  1.  1.  1.  0.  0.  1.  1.  0.  1.  1.  0.\n",
        "  1.  1.  0.  1.  0.  0.  1.  1.  1.  0.  0.  0.  1.  1.  0.  1.  0.  0.\n",
        "  0.  1.  0.  0.  1.  0.  1.  0.  0.  0.  1.  1.  1.  1.  1.  1.  1.  0.\n",
        "  0.  0.  1.  0.  1.  1.  1.  0.  0.  0.  0.  0.  1.  0.  1.  1.  0.  1.\n",
        "  1.  1.  1.  1.  1.  1.  0.  0.  1.  1.  1.  1.  1.  1.  0.  1.  1.  0.\n",
        "  1.  1.  0.  1.  1.  1.  1.  1.  1.  1.]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n",
        "argmax\n"
       ]
      }
     ],
     "prompt_number": 60
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y_pred"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'y_pred' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-14-5a040a10fabc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;31mNameError\u001b[0m: name 'y_pred' is not defined"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import theano.tensor as T\n",
      "from theano import function\n",
      "\n",
      "x = T.dmatrix('x')\n",
      "s = 1 / (1 + T.exp(-x))\n",
      "logistic = function([x], s)\n",
      "logistic([[0, 1], [-1, -2]])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 3,
       "text": [
        "array([[ 0.5       ,  0.73105858],\n",
        "       [ 0.26894142,  0.11920292]])"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import scipy.io as io\n",
      "        \n",
      "print '... cargando datos'\n",
      "data=io.loadmat('dataLR.mat',squeeze_me=True)\n",
      "dataIn=data['data'][:,0:2].astype(float)\n",
      "dataOut = data['data'][:,2].astype(int)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "... cargando datos\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "\n",
      "x=np.zeros((100,2))\n",
      "y=np.zeros((100,2))\n",
      "for i in range(100):\n",
      "    if (dataOut[i]==1):\n",
      "        x[i,:]=dataIn[i,:]\n",
      "    else:\n",
      "        y[i,:]=dataIn[i,:]\n",
      "\n",
      "plt.plot(x[:,0],x[:,1],'ro', y[:,0],y[:,1],'go')\n",
      "plt.axis([25,100,25,100])\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy\n",
      "import theano\n",
      "import theano.tensor as T\n",
      "rng = numpy.random\n",
      "\n",
      "steps=100000\n",
      "feats=2\n",
      "x = T.lmatrix(\"x\")\n",
      "y = T.lvector(\"y\")\n",
      "w = theano.shared(rng.randn(feats))\n",
      "b = theano.shared(0.)\n",
      "print \"Modelo inicial:\"\n",
      "print \"W (tama\u00f1o): \" + repr(w.get_value().shape)\n",
      "print \"b (valor): \" + repr(b.get_value())\n",
      "\n",
      "import scipy.io as io\n",
      "        \n",
      "print '... cargando datos'\n",
      "data=io.loadmat('dataLR.mat',squeeze_me=True)\n",
      "dataIn=data['data'][:,0:2].astype(float)\n",
      "dataOut = data['data'][:,2].astype(int)\n",
      "\n",
      "'''N = 400\n",
      "feats = 2\n",
      "D = (rng.randn(N, feats), rng.randint(size=N, low=0, high=2))\n",
      "#dataIn=D[0]\n",
      "#dataOut=D[1]\n",
      "'''\n",
      "\n",
      "# Construct Theano expression graph\n",
      "p_1 = 1 / (1 + T.exp(-T.dot(x, w) - b))   # Probability that target = 1\n",
      "prediction = p_1 > 0.5                    # The prediction thresholded\n",
      "xent = -y * T.log(p_1) - (1-y) * T.log(1-p_1) # Cross-entropy loss function\n",
      "cost = xent.mean() + 0.01 * (w ** 2).sum()# The cost to minimize\n",
      "gw, gb = T.grad(cost, [w, b])             # Compute the gradient of the cost\n",
      "                                          # (we shall return to this in a\n",
      "                                          # following section of this tutorial\n",
      "        \n",
      "# Compile\n",
      "train = theano.function(\n",
      "          inputs=[x,y],\n",
      "          outputs=[prediction, xent],\n",
      "          updates=((w, w - 0.1 * gw), (b, b - 0.1 * gb)),allow_input_downcast=True)\n",
      "predict = theano.function(inputs=[x], outputs=prediction,allow_input_downcast=True)\n",
      "\n",
      "# Train\n",
      "for i in range(steps):\n",
      "    pred, err = train(dataIn, dataOut)\n",
      "    \n",
      "print \"Valores esperados: \", dataOut\n",
      "pp= predict(dataIn)\n",
      "print \"Valores previstos: \", pp\n",
      "\n",
      "print \"Tasa de acierto: \", map(lambda x,y:x==y, dataOut, predict(dataIn)).count(True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Modelo inicial:\n",
        "W (tama\u00f1o): (2,)\n",
        "b (valor): array(0.0)\n",
        "... cargando datos\n",
        "Valores esperados: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " [0 0 0 1 1 0 1 1 1 1 0 0 1 1 0 1 1 0 1 1 0 1 0 0 1 1 1 0 0 0 1 1 0 1 0 0 0\n",
        " 1 0 0 1 0 1 0 0 0 1 1 1 1 1 1 1 0 0 0 1 0 1 1 1 0 0 0 0 0 1 0 1 1 0 1 1 1\n",
        " 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1]\n",
        "Valores previstos:  [1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
        " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 0 1 1 1\n",
        " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
        "Tasa de acierto:  65\n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "\n",
      "x=np.zeros((100,2))\n",
      "y=np.zeros((100,2))\n",
      "for i in range(100):\n",
      "    if (pp[i]==1):\n",
      "        x[i,:]=dataIn[i,:]\n",
      "    else:\n",
      "        y[i,:]=dataIn[i,:]\n",
      "\n",
      "plt.plot(x[:,0],x[:,1],'ro', y[:,0],y[:,1],'go')\n",
      "plt.axis([25,100,25,100])\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}